{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gun Detection with VGG16\n",
    "\n",
    "This notebook demonstrates how to detecting guns appearing in images or video, using the pre-trained VGG16 deep learning model.\n",
    "\n",
    "To get started, you must have the following requirements installed:\n",
    "\n",
    "1. tensorflow - to preprocess data and work with the VGG16 model\n",
    "2. numpy - to prepare data during testing\n",
    "3. matplotlib - to display images in the Jupyter notebook\n",
    "4. notebook - to run this notebook in a Jupyter server\n",
    "\n",
    "It is recommended to have a virtual environment to isolate these requirements from the rest of your system. This can be done using Python's virtualenv package.\n",
    "\n",
    "First, open a Terminal (Command Prompt on Windows) in the same folder as this notebook and create a virtual environment:\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "```\n",
    "\n",
    "Next, activate the virtual environment. For Windows users:\n",
    "\n",
    "```shell\n",
    ".\\env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "For Linux and MacOS users:\n",
    "\n",
    "```shell\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "Now you can safely install the above requirements in your virtual environment:\n",
    "\n",
    "```shell\n",
    "pip install tensorflow numpy matplotlib notebook\n",
    "```\n",
    "\n",
    "After installation is complete, launch the Jupyter server to edit this notebook:\n",
    "\n",
    "```shell\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following imports are necessary to work with this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Tensorflow and Keras imports\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import load_img\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Lambda, Dense, Flatten, GlobalAveragePooling2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous example, we will be performing **transfer learning** with the **VGG16** pre-trained model from **Keras Applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "For this scenario, we will be using a custom dataset of gun images, derived from multiple sources on the Internet.\n",
    "\n",
    "The images in the dataset have a **high level of variety and diversity**. This ensures that the model can generalize better to locate and identify guns when operationalized in a real-world scenario.\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "First, we'll inspect how the dataset is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./data\u001b[0m\n",
      "├── \u001b[01;34mtrain\u001b[0m\n",
      "│   ├── \u001b[01;34mgun\u001b[0m\n",
      "│   └── \u001b[01;34mnogun\u001b[0m\n",
      "└── \u001b[01;34mvalidation\u001b[0m\n",
      "    ├── \u001b[01;34mgun\u001b[0m\n",
      "    └── \u001b[01;34mno gun\u001b[0m\n",
      "\n",
      "6 directories\n"
     ]
    }
   ],
   "source": [
    "!tree \"./data\" -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the dataset has been divided into 2 sub-directories:\n",
    "\n",
    "1. `train` - training data\n",
    "2. `validation` - validation data (which can be used for testing the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how many images belong to each class in the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "  gun: 60 images\n",
      "  no-gun: 50 images\n",
      "\n",
      "Validation data:\n",
      "  gun: 45 images\n",
      "  no-gun: 50 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_images(directory):\n",
    "    return sum(\n",
    "        1 for _ in os.listdir(directory) if _.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_data(base_dir):\n",
    "    return {\n",
    "        split: {\n",
    "            category: count_images(os.path.join(base_dir, split, category))\n",
    "            for category in [\"gun\", \"no-gun\"]\n",
    "        }\n",
    "        for split in [\"train\", \"validation\"]\n",
    "    }\n",
    "\n",
    "\n",
    "base_dir = \"./data\"\n",
    "data_summary = summarize_data(base_dir)\n",
    "\n",
    "for split, categories in data_summary.items():\n",
    "    print(f\"{split.capitalize()} data:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"  {category}: {count} images\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have a very limited number of images to work with in our dataset, since we built this dataset from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "When training deep learning models using images, it's always better to use lots of data, especially images with slight variations in them, which can help the model generalize better.\n",
    "\n",
    "But since we have limited data available, we can apply **image augmentation** to expand our dataset.\n",
    "\n",
    "Image augmentation is the process of applying different transformations on our existing images, which results in multiple transformed copies of the same image. Each copy differs from its original image through minor differences created using rotation, flipping, shift etc.\n",
    "\n",
    "Keras allows us to do this using the `ImageDataGenerator` class. In our case, we can augment both the training and testing data with different augmentation criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a data generator object to augment data\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the images using the `flow_from_directory()` method in the data generator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 2 classes.\n",
      "Found 19 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the training data via the data generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=\"./data/train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "# Load the validation data via the data generator\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    directory=\"./data/validation\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "We will using VGG16 as the **base model** to train a custom model that is capable of identifying guns.\n",
    "\n",
    "Let's initialize the base model first, and freeze its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base VGG16 model\n",
    "base_model = VGG16(\n",
    "    weights=\"imagenet\",         # Load the weights trained on the \"ImageNet\" dataset\n",
    "    include_top=False,          # Exclude the final dense layer (to add new trainable layers)\n",
    "    input_shape=(224, 224, 3)   # Specify input image properties, i.e. 224px by 224px, 3 color channel (RGB)\n",
    ")\n",
    "\n",
    "# Freeze base model to avoid re-training\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above base model and the `Sequential` class from Keras, we can create the architecture of our custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 512)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14846273 (56.63 MB)\n",
      "Trainable params: 131585 (514.00 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build custom model using base VGG16 model\n",
    "model = Sequential([\n",
    "    # The base VGG16 model\n",
    "    base_model,\n",
    "    \n",
    "    # Reduce spatial dimensions of the feature maps created by the convolutional base model\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Learn high-level features from the data having reduced dimensions\n",
    "    Dense(\n",
    "        units=256,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "\n",
    "    # Regularize the model, by forcing it to learn robust features\n",
    "    Dropout(rate=0.5),\n",
    "\n",
    "    # Fully-connected layer to perform the final binary classification\n",
    "    Dense(\n",
    "        units=1,\n",
    "        activation=\"sigmoid\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "# View the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (configure the model for training)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin training the model using the training data generator created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.5272 - accuracy: 0.7727 - val_loss: 0.4879 - val_accuracy: 0.8947\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.5616 - accuracy: 0.7273 - val_loss: 0.4789 - val_accuracy: 0.8421\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4576 - accuracy: 0.8182 - val_loss: 0.4227 - val_accuracy: 0.9474\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4623 - accuracy: 0.8068 - val_loss: 0.3790 - val_accuracy: 0.9474\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4367 - accuracy: 0.8295 - val_loss: 0.3653 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 12s 5s/step - loss: 0.3826 - accuracy: 0.8864 - val_loss: 0.3576 - val_accuracy: 0.9474\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4099 - accuracy: 0.8636 - val_loss: 0.3556 - val_accuracy: 0.9474\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.3650 - accuracy: 0.9091 - val_loss: 0.3490 - val_accuracy: 0.8421\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 12s 5s/step - loss: 0.3721 - accuracy: 0.8750 - val_loss: 0.3098 - val_accuracy: 0.9474\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3491 - accuracy: 0.8864 - val_loss: 0.3346 - val_accuracy: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7bf9f8561e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the model to use it in practical scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravindu-aratchige/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model (to maintain model persistence)\n",
    "model.save(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
